# Tau Bench Training Configuration
# Based on SkyRL ppo_base_config.yaml with tau_bench specific settings

defaults:
  - _self_
  - deepspeed_config@deepspeed_config.train: train
  - deepspeed_config@deepspeed_config.eval: eval
  - skyrl_gym_config: tau_bench

data:
  train_data: ["data/tau_bench_multi/train.parquet"]
  val_data: ["data/tau_bench_multi/validation.parquet"]
  retail_val_data: ["data/tau_bench_retail/validation.parquet"]  # Retail validation for comparison
  test_data:  # Test sets for final evaluation
    retail: "data/tau_bench_test/retail_test.parquet"
    airline: "data/tau_bench_test/airline_test.parquet"

trainer:
  placement:
    colocate_all: true
    colocate_policy_ref: true
    colocate_critic_reward: false
    policy_num_nodes: 1
    policy_num_gpus_per_node: 8
    critic_num_nodes: 1
    critic_num_gpus_per_node: 8
    ref_num_nodes: 1
    ref_num_gpus_per_node: 8
    reward_num_nodes: 1
    reward_num_gpus_per_node: 8
  
  sequence_parallel_backend: "ulysses"
  strategy: fsdp2
  
  policy:
    model:
      path: "Qwen/Qwen2.5-3B-Instruct"  # Upgraded to more powerful 3B model
    deepspeed_config: ${deepspeed_config.train}
    optimizer_config:
      lr: 5.0e-7  # Reduced from 1.0e-6 for more stable learning
      adam_betas: [0.9, 0.999]
      weight_decay: 5e-2  # Increased from 1e-2 for stronger regularization
      max_grad_norm: 0.5  # Reduced from 1.0 for more conservative gradient updates
      offload_after_step: true
      num_warmup_steps: 50  # Added warmup for stability
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true
      fsdp_size: -1
    sequence_parallel_size: 1
    use_torch_compile: false
    record_memory: false
  
  ref:
    model:
      path: "Qwen/Qwen2.5-3B-Instruct"  # Updated reference model to match policy
    sequence_parallel_size: 1
    deepspeed_config: ${deepspeed_config.eval}
    fsdp_config:
      cpu_offload: true
      reshard_after_forward: true
      fsdp_size: -1
      
  critic:
    model:
      path: null
    deepspeed_config: ${deepspeed_config.train}
    optimizer_config:
      lr: 5.0e-6
      adam_betas: [0.9, 0.999]
      weight_decay: 1e-2
      max_grad_norm: 1.0
      offload_after_step: true
      num_warmup_steps: 0
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true
      fsdp_size: -1
    sequence_parallel_size: 1
    
  reward:
    model:
      path: null
    deepspeed_config: ${deepspeed_config.eval}
    fsdp_config:
      cpu_offload: true
      reshard_after_forward: true
      fsdp_size: -1
    sequence_parallel_size: 1
    
  algorithm:
    advantage_estimator: "grpo"
    kl_target: null
    init_kl_coef: 0.0
    use_kl_estimator_k3: true
    use_abs_kl: false
    use_kl_in_reward: false
    use_kl_loss: true
    kl_loss_coef: 0.01  # Increased from 0.001 for stronger regularization
    advantage_batch_normalize: false
    value_head_prefix: "value_head"
    ppo_loss_type: "regular"
    loss_reduction: "token_mean"
    lambd: 1.0
    gamma: 1.0
    eps_clip_low: 0.1  # Reduced from 0.2 for more conservative updates
    eps_clip_high: 0.1  # Reduced from 0.2 for more conservative updates
    clip_ratio_c: 3.0
    value_clip: 0.1  # Reduced from 0.2 for more stable value updates
    normalize_reward: true

  gradient_checkpointing: true
  gradient_checkpointing_use_reentrant: false
  seed: 42
  resume_mode: latest
  resume_path: null
  ckpt_path: "${oc.env:HOME}/ckpts/tau_bench"
  max_ckpts_to_keep: 3
  ckpt_interval: 10
  hf_save_interval: 20
  export_path: "${oc.env:HOME}/exports/tau_bench"
  bf16: true
  epochs: 50
  update_epochs_per_batch: 1
  train_batch_size: 256  # Suitable for 3B model
  policy_mini_batch_size: 64  # Suitable for 3B model
  critic_mini_batch_size: 64  # Suitable for 3B model
  micro_train_batch_size_per_gpu: 1
  micro_forward_batch_size_per_gpu: 2  # Suitable for 3B model
  update_ref_every_epoch: false
  # use_sample_packing: true
  use_sample_packing: false  # Disabled because flash attention is not available
  eval_batch_size: 128  # Suitable for 3B model
  eval_before_train: true
  eval_interval: 5
  max_prompt_length: 16384  # Increased to handle very long tool-rich prompts
  flash_attn: false
  disable_fast_tokenizer: false
  target_modules: "all-linear"
  use_orm_score: false
  project_name: "tau_bench_rl"
  run_name: "tau_bench_qwen2_5_3b_v1"  # Updated for new model
  logger: "wandb"
  dump_data_batch: false
  dump_eval_results: true

generator:
  model_dtype: "bfloat16"
  run_engines_locally: true
  num_inference_engines: 2
  backend: "vllm"
  weight_sync_backend: "nccl"
  inference_engine_tensor_parallel_size: 4
  n_samples_per_prompt: 3
  async_engine: true
  batched: false  # Important for multi-turn conversations
  max_input_length: 16384  # Increased from 8192 to handle very long tool-rich prompts
  vllm_v1_disable_multiproc: true
  enable_prefix_caching: true
  enable_chunked_prefill: true
  # max_num_batched_tokens: 8192
  max_num_batched_tokens: 32768  # Increased to handle longer sequences with high GPU utilization
  enforce_eager: false
  # gpu_memory_utilization: 0.7  # Suitable for 3B model
  gpu_memory_utilization: 0.95  # Increased to allow full context length usage
  max_num_seqs: 256
  remote_inference_engine_urls: ["127.0.0.1:8001"]
  max_turns: 20  # Multi-turn conversations
  
  override_existing_update_group: "auto"
  
  # Sampling parameters for generation
  sampling_params:
    max_generate_length: 1024  # Reduced from 2048 to balance context window
    temperature: 0.8  # Increased from 0.7 for more exploration
    top_p: 0.95  # Increased from 0.9 for more diversity
    min_p: 0.0
    top_k: -1

  # Use conversation-based format for multi-turn
  use_conversation_multi_turn: true
  
  # Native tool calling (optional - requires VLLM chat method)
  use_native_tool_calling: true  # Set to true to enable VLLM native tool calling

  # Evaluation sampling parameters
  eval_sampling_params:
    max_generate_length: 1024  # Reduced to match training config
    temperature: 0.0
    top_p: 1.0
    min_p: 0.0
    top_k: -1

  # Number of samples per prompt for evaluation
  eval_n_samples_per_prompt: 1

  # Zero reward for non-stop generations
  zero_reward_on_non_stop: true

environment:
  env_class: "tau_bench"
  skyrl_gym:
    max_env_workers: 16
    tau_bench:
      user_strategy: "llm"
      user_model: "gpt-4o"
      user_provider: "openai"
      max_turns: 20
      use_native_tool_calling: true  # Enable native VLLM tool calling with structured responses 