_wandb:
    value:
        cli_version: 0.21.0
        e:
            ujnles5ax0v5pngsa9lov7c8ggzjtisp:
                args:
                    - --node-ip-address=240.20.60.106
                    - --node-manager-port=43229
                    - --object-store-name=/mnt/tmp/ray/session_2025-07-28_16-59-06_340723_15592/sockets/plasma_store
                    - --raylet-name=/mnt/tmp/ray/session_2025-07-28_16-59-06_340723_15592/sockets/raylet
                    - --redis-address=None
                    - --metrics-agent-port=62519
                    - --logging-rotate-bytes=536870912
                    - --logging-rotate-backup-count=5
                    - --runtime-env-agent-port=51708
                    - --gcs-address=240.20.60.106:40609
                    - --session-name=session_2025-07-28_16-59-06_340723_15592
                    - --temp-dir=/mnt/tmp/ray
                    - --webui=
                    - --cluster-id=18cc296ac388b0b5f5b8822eedf105b0e3b9fde4af135bdf9616c313
                    - --startup-token=96
                    - --worker-launch-time-ms=1753721949582
                    - --node-id=0855f5c1a7b06d8b9ac8b8800b6f8acbc3cf339ebeb3f01e2a71d83e
                    - --runtime-env-hash=1417985805
                email: mert_cemri@berkeley.edu
                executable: /root/miniconda3/envs/ctu/bin/python
                git:
                    commit: dba567a0b68609b70a951f9c6f57e76a01f2b669
                    remote: https://github.com/mert-cemri/CTU-Agent-v0.git
                host: bolt-udx594w68d-ng3r4y5r7c
                os: Linux-6.6.72+-x86_64-with-glibc2.35
                program: /root/miniconda3/envs/ctu/lib/python3.12/site-packages/ray/_private/workers/default_worker.py
                python: CPython 3.12.11
                root: /root/CTU-Agent-v0/training
                startedAt: "2025-07-28T16:59:25.223055Z"
                writerId: ujnles5ax0v5pngsa9lov7c8ggzjtisp
        m: []
        python_version: 3.12.11
        t:
            "1":
                - 1
                - 11
                - 30
                - 49
                - 50
                - 51
                - 71
                - 98
                - 105
            "2":
                - 1
                - 11
                - 30
                - 49
                - 50
                - 51
                - 71
                - 98
                - 105
            "3":
                - 2
                - 13
                - 16
            "4": 3.12.11
            "5": 0.21.0
            "6": 4.54.0
            "12": 0.21.0
            "13": linux-x86_64
data:
    value:
        train_data:
            - data/tau_bench_retail/train.parquet
        val_data:
            - data/tau_bench_retail/validation.parquet
deepspeed_config:
    value:
        eval:
            gradient_clipping: 1
            prescale_gradient: false
            wall_clock_breakdown: false
            zero_optimization:
                offload_param:
                    device: cpu
                    pin_memory: true
                stage: 3
                stage3_param_persistence_threshold: auto
        train:
            data_types:
                grad_accum_dtype: fp32
            disable_trace_cache: false
            gradient_clipping: 1
            prescale_gradient: false
            wall_clock_breakdown: false
            zero_optimization:
                offload_optimizer:
                    device: none
                    pin_memory: true
                offload_param:
                    device: none
                reduce_bucket_size: auto
                round_robin_gradients: true
                stage: 3
                stage3_max_live_parameters: auto
                stage3_max_reuse_distance: auto
                stage3_param_persistence_threshold: auto
                stage3_prefetch_bucket_size: auto
                sub_group_size: auto
                zero_hpz_partition_size: 1
                zero_quantized_gradients: false
                zero_quantized_weights: false
environment:
    value:
        env_class: tau_bench
        skyrl_gym:
            max_env_workers: 16
            tau_bench:
                cache_environments: true
                log_conversations: true
                log_tool_calls: true
                max_turns: 10
                parallel_user_sim: false
                timeout: 120
                use_native_tool_calling: true
                user_model: gpt-4o
                user_provider: openai
                user_strategy: llm
generator:
    value:
        async_engine: true
        backend: vllm
        batched: false
        enable_chunked_prefill: true
        enable_prefix_caching: true
        enforce_eager: true
        eval_n_samples_per_prompt: 1
        eval_sampling_params:
            max_generate_length: 1024
            min_p: 0
            temperature: 0
            top_k: -1
            top_p: 1
        gpu_memory_utilization: 0.7
        inference_engine_tensor_parallel_size: 4
        max_input_length: 8192
        max_num_batched_tokens: 8192
        max_num_seqs: 256
        max_turns: 20
        model_dtype: bfloat16
        n_samples_per_prompt: 3
        num_inference_engines: 2
        override_existing_update_group: force_new
        remote_inference_engine_urls:
            - 127.0.0.1:8001
        run_engines_locally: true
        sampling_params:
            max_generate_length: 1024
            min_p: 0
            temperature: 1
            top_k: -1
            top_p: 0.9
        use_conversation_multi_turn: true
        use_native_tool_calling: true
        vllm_v1_disable_multiproc: true
        weight_sync_backend: nccl
        zero_reward_on_non_stop: true
trainer:
    value:
        algorithm:
            advantage_batch_normalize: false
            advantage_estimator: grpo
            clip_ratio_c: 3
            eps_clip_high: 0.2
            eps_clip_low: 0.2
            gamma: 1
            init_kl_coef: 0
            kl_loss_coef: 0.001
            kl_target: null
            lambd: 1
            loss_reduction: token_mean
            normalize_reward: true
            ppo_loss_type: regular
            use_abs_kl: false
            use_kl_estimator_k3: true
            use_kl_in_reward: false
            use_kl_loss: true
            value_clip: 0.2
            value_head_prefix: value_head
        bf16: true
        ckpt_interval: 10
        ckpt_path: /root/ckpts/tau_bench/Qwen/Qwen2.5-3B-Instruct_v0
        critic:
            deepspeed_config:
                data_types:
                    grad_accum_dtype: fp32
                disable_trace_cache: false
                gradient_clipping: 1
                prescale_gradient: false
                wall_clock_breakdown: false
                zero_optimization:
                    offload_optimizer:
                        device: none
                        pin_memory: true
                    offload_param:
                        device: none
                    reduce_bucket_size: auto
                    round_robin_gradients: true
                    stage: 3
                    stage3_max_live_parameters: auto
                    stage3_max_reuse_distance: auto
                    stage3_param_persistence_threshold: auto
                    stage3_prefetch_bucket_size: auto
                    sub_group_size: auto
                    zero_hpz_partition_size: 1
                    zero_quantized_gradients: false
                    zero_quantized_weights: false
            fsdp_config:
                cpu_offload: false
                fsdp_size: -1
                reshard_after_forward: true
            model:
                path: null
            optimizer_config:
                adam_betas:
                    - 0.9
                    - 0.999
                lr: 5e-06
                max_grad_norm: 1
                num_warmup_steps: 0
                offload_after_step: true
                weight_decay: 0.01
            sequence_parallel_size: 1
        critic_mini_batch_size: 64
        disable_fast_tokenizer: false
        dump_data_batch: false
        dump_eval_results: true
        epochs: 50
        eval_batch_size: 64
        eval_before_train: true
        eval_interval: 2
        export_path: /root/exports/tau_bench
        flash_attn: false
        gradient_checkpointing: true
        gradient_checkpointing_use_reentrant: false
        hf_save_interval: 20
        logger: wandb
        max_ckpts_to_keep: 3
        max_prompt_length: 8192
        micro_forward_batch_size_per_gpu: 1
        micro_train_batch_size_per_gpu: 1
        placement:
            colocate_all: true
            colocate_critic_reward: false
            colocate_policy_ref: true
            critic_num_gpus_per_node: 8
            critic_num_nodes: 1
            policy_num_gpus_per_node: 8
            policy_num_nodes: 1
            ref_num_gpus_per_node: 8
            ref_num_nodes: 1
            reward_num_gpus_per_node: 8
            reward_num_nodes: 1
        policy:
            deepspeed_config:
                data_types:
                    grad_accum_dtype: fp32
                disable_trace_cache: false
                gradient_clipping: 1
                prescale_gradient: false
                wall_clock_breakdown: false
                zero_optimization:
                    offload_optimizer:
                        device: none
                        pin_memory: true
                    offload_param:
                        device: none
                    reduce_bucket_size: auto
                    round_robin_gradients: true
                    stage: 3
                    stage3_max_live_parameters: auto
                    stage3_max_reuse_distance: auto
                    stage3_param_persistence_threshold: auto
                    stage3_prefetch_bucket_size: auto
                    sub_group_size: auto
                    zero_hpz_partition_size: 1
                    zero_quantized_gradients: false
                    zero_quantized_weights: false
            fsdp_config:
                cpu_offload: false
                fsdp_size: -1
                reshard_after_forward: true
            model:
                path: Qwen/Qwen2.5-3B-Instruct
            optimizer_config:
                adam_betas:
                    - 0.9
                    - 0.999
                lr: 5e-06
                max_grad_norm: 1
                num_warmup_steps: 100
                offload_after_step: true
                weight_decay: 0.01
            record_memory: false
            sequence_parallel_size: 1
            use_torch_compile: false
        policy_mini_batch_size: 32
        project_name: tau_bench_rl
        ref:
            deepspeed_config:
                gradient_clipping: 1
                prescale_gradient: false
                wall_clock_breakdown: false
                zero_optimization:
                    offload_param:
                        device: cpu
                        pin_memory: true
                    stage: 3
                    stage3_param_persistence_threshold: auto
            fsdp_config:
                cpu_offload: true
                fsdp_size: -1
                reshard_after_forward: true
            model:
                path: Qwen/Qwen2.5-3B-Instruct
            sequence_parallel_size: 1
        resume_mode: latest
        resume_path: null
        reward:
            deepspeed_config:
                gradient_clipping: 1
                prescale_gradient: false
                wall_clock_breakdown: false
                zero_optimization:
                    offload_param:
                        device: cpu
                        pin_memory: true
                    stage: 3
                    stage3_param_persistence_threshold: auto
            fsdp_config:
                cpu_offload: true
                fsdp_size: -1
                reshard_after_forward: true
            model:
                path: null
            sequence_parallel_size: 1
        run_name: tau_bench_qwen2_5_3b_20250728_165857
        seed: 42
        sequence_parallel_backend: ulysses
        strategy: fsdp2
        target_modules: all-linear
        train_batch_size: 128
        update_epochs_per_batch: 1
        update_ref_every_epoch: false
        use_orm_score: false
        use_sample_packing: false
